
HPKB Meeting, January 1999, Austin, TX
--------------------------------------

* PROTEGE (M. Musen)
  - New KR that is OKBC compliant
  - The importance of custom widgets
  - May collaborate to integrate our tools

* Component libraries (B. Porter)
  - ex: conversion, production, treatment, expansion (of substances)
  - events, objects, shapes & places, compound objects
 
-> how could we get & use these libraries

* Dialogue behavior for KA (S. Luperfoy):

 - Two stages
    1. Knowledge entry: query, assert, test
    2. Knowledge-based assisted problem-solving

 - Pragmatics on input
    ex: can/should user really delete that?  

-> could help us understand and model the KA process, 
     incorporate dialogue structure into our tools

* McGuinness on explanation:
  - Interesting: found that users do not always know how to pose 
     follow-up questions and instead it works better if the system
     itself proposes follow-up questions

* McCarthy
  - Elaboration tolerance
     "adding new sentences is the easiest way to extend a theory"
     Designing a KB so that it can be changed easily

-> could this help us design easier/harder KA tasks

* Analogy (K. Forbus):
  Three uses of analogy
   - as an organizer, e.g., "hey, have you thought of this?"
   - as a microscope, i.e., highlight differences
   - as predictors, i.e., given cumulated knowledge, overcome biases 
  Basic functions: retrieve-case, find-similarities, find-differences

-> should try to generalize small analogy-like functions in our tools
-> could we use PowerLoom's partial matcher to do these things in our KA tool

* GMU claims (G. Tecuci):
  - rapid learning
  - increased assistance to SMEs
  - rules have high quality
  - rules do high-performance problem-solving
  Also: "easier to do X than Y" claims

-> we should check how we do on these (e.g., higher quality k with our tools)

* Evaluations (P. Cohen):
  1. measure reuse of prior knowledge
  2. measure how k engineers spend their time
  3. measure how KA tools help (for different kinds of users)
  4. performance of resulting system

-> we need to try to instrument & evaluate as much as possible

* Keeping logs of KE activities (D. Lenat): 

  - Size of task
  - Check sources
  - Search the KB
  - Add new terms
  - Add/edit rules
  - Mentored
  - Kicking system (look at the KB doing the wrong thing)

-> should do this

* COA CP & KA CCE evaluation (E. Jones):

   Performance evaluation:
	- June 1-15: dry-run test questions
	- scores released in late June
	- main evaluation: July to mid-August 
	- several levels of difficulty:
		- a new test question
		- slightly modify scenario
		- new scenario similar to original one
		- new scenario very different from original one

   KA CCE: 
	- Mid-March: mini dry-run
	- Mid-April: finalize overall experiment design
		(discuss this at SME/Integration meeting)
	- main evaluation July/early August (during the performance evaluation)


